{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tcs_only_modeling_abt.tsv\", sep='\\t')\n",
    "data = df.drop([\"uid\", \"tech_obs_id\", \"app_bank_nm\", \"tech_dog_type\", \"trg_utilization\", \"tech_target_source\", \"tech_target_d\"],\n",
    "               axis = 1)\n",
    "data = data.drop(['var_0{}'.format(i) for i in range(63, 73 + 1)], axis = 1)\n",
    "data = data.dropna()\n",
    "data = pd.get_dummies(data)\n",
    "y1 = data['trg_pd']\n",
    "y2 = data['trg_grace']\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = data.columns)\n",
    "data = data.drop(columns=['trg_pd', 'trg_grace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_001</th>\n",
       "      <th>var_002</th>\n",
       "      <th>var_003</th>\n",
       "      <th>var_004</th>\n",
       "      <th>var_005</th>\n",
       "      <th>var_006</th>\n",
       "      <th>var_007</th>\n",
       "      <th>var_008</th>\n",
       "      <th>var_009</th>\n",
       "      <th>var_010</th>\n",
       "      <th>...</th>\n",
       "      <th>var_062_mts</th>\n",
       "      <th>var_062_orient</th>\n",
       "      <th>var_062_pochta</th>\n",
       "      <th>var_062_renaissance</th>\n",
       "      <th>var_062_rosbank</th>\n",
       "      <th>var_062_sbrf</th>\n",
       "      <th>var_062_tinkoff</th>\n",
       "      <th>var_062_ubrr</th>\n",
       "      <th>var_062_uralsib</th>\n",
       "      <th>var_062_vtb24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.167919</td>\n",
       "      <td>0.673893</td>\n",
       "      <td>-0.767992</td>\n",
       "      <td>-0.073527</td>\n",
       "      <td>1.374671</td>\n",
       "      <td>-0.070758</td>\n",
       "      <td>-0.480494</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>0.655862</td>\n",
       "      <td>-0.166119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>-0.058819</td>\n",
       "      <td>-0.137902</td>\n",
       "      <td>-0.035815</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.655344</td>\n",
       "      <td>-0.087093</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>-0.141723</td>\n",
       "      <td>-0.360996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.167919</td>\n",
       "      <td>0.673893</td>\n",
       "      <td>-0.767992</td>\n",
       "      <td>-0.073527</td>\n",
       "      <td>-0.727447</td>\n",
       "      <td>-0.070758</td>\n",
       "      <td>-0.480494</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>0.655862</td>\n",
       "      <td>-0.166119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>-0.058819</td>\n",
       "      <td>-0.137902</td>\n",
       "      <td>-0.035815</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.655344</td>\n",
       "      <td>-0.087093</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>-0.141723</td>\n",
       "      <td>-0.360996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.167919</td>\n",
       "      <td>0.673893</td>\n",
       "      <td>-0.767992</td>\n",
       "      <td>-0.073527</td>\n",
       "      <td>-0.727447</td>\n",
       "      <td>-0.070758</td>\n",
       "      <td>-0.480494</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>0.655862</td>\n",
       "      <td>-0.166119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>-0.058819</td>\n",
       "      <td>-0.137902</td>\n",
       "      <td>-0.035815</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.655344</td>\n",
       "      <td>-0.087093</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>-0.141723</td>\n",
       "      <td>-0.360996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.167919</td>\n",
       "      <td>0.673893</td>\n",
       "      <td>-0.767992</td>\n",
       "      <td>-0.073527</td>\n",
       "      <td>1.374671</td>\n",
       "      <td>-0.070758</td>\n",
       "      <td>-0.480494</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>0.655862</td>\n",
       "      <td>-0.166119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>-0.058819</td>\n",
       "      <td>-0.137902</td>\n",
       "      <td>-0.035815</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>-1.525916</td>\n",
       "      <td>-0.087093</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>-0.141723</td>\n",
       "      <td>2.770113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.167919</td>\n",
       "      <td>-1.483916</td>\n",
       "      <td>-0.767992</td>\n",
       "      <td>-0.073527</td>\n",
       "      <td>-0.727447</td>\n",
       "      <td>-0.070758</td>\n",
       "      <td>-0.480494</td>\n",
       "      <td>-0.125601</td>\n",
       "      <td>-1.524711</td>\n",
       "      <td>-0.166119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>-0.058819</td>\n",
       "      <td>-0.137902</td>\n",
       "      <td>-0.035815</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.655344</td>\n",
       "      <td>-0.087093</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>-0.141723</td>\n",
       "      <td>-0.360996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    var_001   var_002   var_003   var_004   var_005   var_006   var_007  \\\n",
       "0 -0.167919  0.673893 -0.767992 -0.073527  1.374671 -0.070758 -0.480494   \n",
       "1 -0.167919  0.673893 -0.767992 -0.073527 -0.727447 -0.070758 -0.480494   \n",
       "2 -0.167919  0.673893 -0.767992 -0.073527 -0.727447 -0.070758 -0.480494   \n",
       "3 -0.167919  0.673893 -0.767992 -0.073527  1.374671 -0.070758 -0.480494   \n",
       "4 -0.167919 -1.483916 -0.767992 -0.073527 -0.727447 -0.070758 -0.480494   \n",
       "\n",
       "    var_008   var_009   var_010  ...  var_062_mts  var_062_orient  \\\n",
       "0 -0.125601  0.655862 -0.166119  ...    -0.046598       -0.058819   \n",
       "1 -0.125601  0.655862 -0.166119  ...    -0.046598       -0.058819   \n",
       "2 -0.125601  0.655862 -0.166119  ...    -0.046598       -0.058819   \n",
       "3 -0.125601  0.655862 -0.166119  ...    -0.046598       -0.058819   \n",
       "4 -0.125601 -1.524711 -0.166119  ...    -0.046598       -0.058819   \n",
       "\n",
       "   var_062_pochta  var_062_renaissance  var_062_rosbank  var_062_sbrf  \\\n",
       "0       -0.137902            -0.035815        -0.126636      0.655344   \n",
       "1       -0.137902            -0.035815        -0.126636      0.655344   \n",
       "2       -0.137902            -0.035815        -0.126636      0.655344   \n",
       "3       -0.137902            -0.035815        -0.126636     -1.525916   \n",
       "4       -0.137902            -0.035815        -0.126636      0.655344   \n",
       "\n",
       "   var_062_tinkoff  var_062_ubrr  var_062_uralsib  var_062_vtb24  \n",
       "0        -0.087093     -0.087736        -0.141723      -0.360996  \n",
       "1        -0.087093     -0.087736        -0.141723      -0.360996  \n",
       "2        -0.087093     -0.087736        -0.141723      -0.360996  \n",
       "3        -0.087093     -0.087736        -0.141723       2.770113  \n",
       "4        -0.087093     -0.087736        -0.141723      -0.360996  \n",
       "\n",
       "[5 rows x 471 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63230, 471)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(torch.nn.Module):\n",
    "    def __init__(self, encoding_len):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(471, encoding_len)\n",
    "        self.ac1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(encoding_len, 471)\n",
    "        self.ac2 = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2(x)\n",
    "        return x\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y2, test_size=0.3, stratify=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.float()\n",
    "X_test = X_test.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X_train, X_test, net, losses):\n",
    "    batch_size = 100\n",
    "    loss_history = []\n",
    "    losses = loss_history\n",
    "    loss = torch.nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        order = numpy.random.permutation(len(X_train))\n",
    "        for start_index in range(0, len(X_train), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            X_batch = X_train[order[start_index : start_index + batch_size]]\n",
    "            preds = net.forward(X_batch)\n",
    "            loss_val = loss(preds, X_batch)\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        test_pred = net.forward(X_test)\n",
    "        loss_history.append(loss(test_pred, X_test))\n",
    "        print(loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = EncoderNet(100)\n",
    "net = net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=3.0e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7177, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5898, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5276, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4850, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4577, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4416, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4279, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4195, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4107, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4047, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3990, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3963, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3931, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3893, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3856, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3845, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3834, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3815, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3786, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3779, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3762, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3753, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3740, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3738, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3721, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3714, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3705, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3704, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3698, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3688, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3681, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3678, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3674, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3665, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3685, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3666, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3655, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3650, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3648, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3646, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3641, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3663, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3662, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3637, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3632, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3627, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3644, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3624, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3621, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3635, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3619, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3612, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3610, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3609, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3614, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3607, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3615, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3601, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3627, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3625, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3599, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3632, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3600, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3591, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3590, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3588, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3584, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3582, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3582, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3580, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3582, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-311ed6240cfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-2804b641f414>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(X_train, X_test, net, losses)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mloss_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "learn(X_train, X_test, net, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2383b62d0b8>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANoklEQVR4nO3dUYic13mH8edvqWoodZxSbSBIitehMkSYgs1iXAKNg90i60K6cYMEJk0RFknr9CKh4OLiBuWqDq0hoDYRrXETiB0lF8kSFARNbVxM5GqNHceSUdkqTrTI1JvE9Y1xbNG3FzMJw2p251tpdkd79PxAMN98RzPv0a4ej2d2NKkqJEkb33WTHkCSNB4GXZIaYdAlqREGXZIaYdAlqRGbJ3XHW7durenp6UndvSRtSM8///zPqmpq2LmJBX16epq5ublJ3b0kbUhJfrLcOZ9ykaRGGHRJaoRBl6RGGHRJaoRBl6RGjAx6kseSvJ7k5WXOJ8mXkswneSnJbeMfU5I0SpdH6I8Du1c4fw+ws//rEPBPVz6WJGm1Rga9qp4BfrHCkn3AV6vnJPC+JB8Y14CSpG7G8Rz6NuD8wPFC/7pLJDmUZC7J3OLi4hjuWpL0K+MIeoZcN/RTM6rqaFXNVNXM1NTQd65Kki7TOIK+AOwYON4OXBjD7UqSVmEcQZ8FPtH/aZc7gDer6rUx3K4kaRVG/uNcSZ4A7gS2JlkA/hb4DYCq+jJwHNgDzANvAX+2VsNKkpY3MuhVdWDE+QL+YmwTSZIui+8UlaRGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGdAp6kt1JziaZT/LgkPMfTPJUkheSvJRkz/hHlSStZGTQk2wCjgD3ALuAA0l2LVn2N8CxqroV2A/847gHlSStrMsj9NuB+ao6V1XvAE8C+5asKeC9/cs3ABfGN6IkqYsuQd8GnB84XuhfN+jzwH1JFoDjwGeG3VCSQ0nmkswtLi5exriSpOV0CXqGXFdLjg8Aj1fVdmAP8LUkl9x2VR2tqpmqmpmamlr9tJKkZXUJ+gKwY+B4O5c+pXIQOAZQVT8A3gNsHceAkqRuugT9FLAzyU1JttB70XN2yZqfAncBJPkwvaD7nIokraORQa+qi8ADwAngFXo/zXI6yeEke/vLPgfcn+SHwBPAJ6tq6dMykqQ1tLnLoqo6Tu/FzsHrHh64fAb4yHhHkySthu8UlaRGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJakSnoCfZneRskvkkDy6z5uNJziQ5neTr4x1TkjTK5lELkmwCjgB/BCwAp5LMVtWZgTU7gb8GPlJVbyR5/1oNLEkarssj9NuB+ao6V1XvAE8C+5asuR84UlVvAFTV6+MdU5I0SpegbwPODxwv9K8bdDNwc5Jnk5xMsnvYDSU5lGQuydzi4uLlTSxJGqpL0DPkulpyvBnYCdwJHAD+Ocn7LvlNVUeraqaqZqamplY7qyRpBV2CvgDsGDjeDlwYsuY7VfVuVf0YOEsv8JKkddIl6KeAnUluSrIF2A/MLlnzbeBjAEm20nsK5tw4B5UkrWxk0KvqIvAAcAJ4BThWVaeTHE6yt7/sBPDzJGeAp4C/qqqfr9XQkqRLpWrp0+HrY2Zmpubm5iZy35K0USV5vqpmhp3znaKS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1IhOQU+yO8nZJPNJHlxh3b1JKsnM+EaUJHUxMuhJNgFHgHuAXcCBJLuGrLse+EvguXEPKUkarcsj9NuB+ao6V1XvAE8C+4as+wLwCPD2GOeTJHXUJejbgPMDxwv9634tya3Ajqr67ko3lORQkrkkc4uLi6seVpK0vC5Bz5Dr6tcnk+uAR4HPjbqhqjpaVTNVNTM1NdV9SknSSF2CvgDsGDjeDlwYOL4euAV4OsmrwB3ArC+MStL66hL0U8DOJDcl2QLsB2Z/dbKq3qyqrVU1XVXTwElgb1XNrcnEkqShRga9qi4CDwAngFeAY1V1OsnhJHvXekBJUjebuyyqquPA8SXXPbzM2juvfCxJ0mr5TlFJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGdAp6kt1JziaZT/LgkPOfTXImyUtJvp/kxvGPKklaycigJ9kEHAHuAXYBB5LsWrLsBWCmqn4f+BbwyLgHlSStrMsj9NuB+ao6V1XvAE8C+wYXVNVTVfVW//AksH28Y0qSRukS9G3A+YHjhf51yzkIfG/YiSSHkswlmVtcXOw+pSRppC5Bz5DraujC5D5gBvjisPNVdbSqZqpqZmpqqvuUkqSRNndYswDsGDjeDlxYuijJ3cBDwEer6pfjGU+S1FWXR+ingJ1JbkqyBdgPzA4uSHIr8BVgb1W9Pv4xJUmjjAx6VV0EHgBOAK8Ax6rqdJLDSfb2l30R+G3gm0leTDK7zM1JktZIl6dcqKrjwPEl1z08cPnuMc8lSVol3ykqSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY3oFPQku5OcTTKf5MEh538zyTf6559LMj3uQSVJKxsZ9CSbgCPAPcAu4ECSXUuWHQTeqKrfAx4F/m7cg0qSVtblEfrtwHxVnauqd4AngX1L1uwD/rV/+VvAXUkyvjElSaN0Cfo24PzA8UL/uqFrquoi8Cbwu0tvKMmhJHNJ5hYXFy9vYknSUF2CPuyRdl3GGqrqaFXNVNXM1NRUl/kkSR11CfoCsGPgeDtwYbk1STYDNwC/GMeAkqRuugT9FLAzyU1JtgD7gdkla2aBP+1fvhf496q65BG6JGntbB61oKouJnkAOAFsAh6rqtNJDgNzVTUL/AvwtSTz9B6Z71/LoSVJlxoZdICqOg4cX3LdwwOX3wb+ZLyjSZJWw3eKSlIjDLokNcKgS1IjDLokNSKT+unCJIvATy7zt28FfjbGcTYC93xtcM/XhivZ841VNfSdmRML+pVIMldVM5OeYz2552uDe742rNWefcpFkhph0CWpERs16EcnPcAEuOdrg3u+NqzJnjfkc+iSpEtt1EfokqQlDLokNeKqDvq1+OHUHfb82SRnkryU5PtJbpzEnOM0as8D6+5NUkk2/I+4ddlzko/3v9ank3x9vWcctw7f2x9M8lSSF/rf33smMee4JHksyetJXl7mfJJ8qf/n8VKS2674TqvqqvxF75/q/W/gQ8AW4IfAriVr/hz4cv/yfuAbk557Hfb8MeC3+pc/fS3sub/ueuAZ4CQwM+m51+HrvBN4Afid/vH7Jz33Ouz5KPDp/uVdwKuTnvsK9/yHwG3Ay8uc3wN8j94nvt0BPHel93k1P0K/Fj+ceuSeq+qpqnqrf3iS3idIbWRdvs4AXwAeAd5ez+HWSJc93w8cqao3AKrq9XWecdy67LmA9/Yv38Cln4y2oVTVM6z8yW37gK9Wz0ngfUk+cCX3eTUHfWwfTr2BdNnzoIP0/gu/kY3cc5JbgR1V9d31HGwNdfk63wzcnOTZJCeT7F636dZGlz1/HrgvyQK9z1/4zPqMNjGr/fs+UqcPuJiQsX049QbSeT9J7gNmgI+u6URrb8U9J7kOeBT45HoNtA66fJ0303va5U56/xf2H0luqar/XePZ1kqXPR8AHq+qv0/yB/Q+Be2Wqvq/tR9vIsber6v5Efq1+OHUXfZMkruBh4C9VfXLdZptrYza8/XALcDTSV6l91zj7AZ/YbTr9/Z3qurdqvoxcJZe4DeqLns+CBwDqKofAO+h949YtarT3/fVuJqDfi1+OPXIPfeffvgKvZhv9OdVYcSeq+rNqtpaVdNVNU3vdYO9VTU3mXHHosv39rfpvQBOkq30noI5t65TjleXPf8UuAsgyYfpBX1xXadcX7PAJ/o/7XIH8GZVvXZFtzjpV4JHvEq8B/gveq+OP9S/7jC9v9DQ+4J/E5gH/hP40KRnXoc9/xvwP8CL/V+zk555rfe8ZO3TbPCfcun4dQ7wD8AZ4EfA/knPvA573gU8S+8nYF4E/njSM1/hfp8AXgPepfdo/CDwKeBTA1/jI/0/jx+N4/vat/5LUiOu5qdcJEmrYNAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa8f+HT9K8XY8HjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
